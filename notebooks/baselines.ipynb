{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W66sxaAa9SsV"
   },
   "outputs": [],
   "source": [
    "! pip install spacy\n",
    "! python -m spacy download en\n",
    "! pip install torchtext==0.2.3\n",
    "! pip install tweet-preprocessor\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AM0k3vFh9Psd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrNRTYpt9dsT"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# from fastai.text import *\n",
    "# import preprocessor as pre\n",
    "# pre.set_options(pre.OPT.URL,pre.OPT.NUMBER,pre.OPT.MENTION,pre.OPT.HASHTAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zfR-LKtx9Psi",
    "outputId": "1a88c4ca-e9f7-4b09-d317-ea5b2c0b5180"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline,linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from fastai.plots import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "EcnzDbcy9Psl",
    "outputId": "0e213316-d2d2-4572-d1d6-f84fdf6e187b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzPFW3np9ii4"
   },
   "outputs": [],
   "source": [
    "PATH=Path(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjZRmbWVASRC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>flair</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>comment</th>\n",
       "      <th>authors</th>\n",
       "      <th>combined_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lost my Job, Sick Mother and Paralysed Dad, In...</td>\n",
       "      <td>1042</td>\n",
       "      <td>g014wc</td>\n",
       "      <td>Hi....It's really tough time for everyone. I r...</td>\n",
       "      <td>sanand_satwik</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g014wc...</td>\n",
       "      <td>132</td>\n",
       "      <td>1.586742e+09</td>\n",
       "      <td>I'm a freelancer. Don't listen to the idiots ...</td>\n",
       "      <td>hashedram diabapp xataari Aashayrao sarcrasti...</td>\n",
       "      <td>Lost my Job, Sick Mother and Paralysed Dad, In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why does the government come with a begging bo...</td>\n",
       "      <td>647</td>\n",
       "      <td>fxofyu</td>\n",
       "      <td>We have floods, terrorist attacks, famines due...</td>\n",
       "      <td>TWO-WHEELER-MAFIA</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fxofyu...</td>\n",
       "      <td>205</td>\n",
       "      <td>1.586448e+09</td>\n",
       "      <td>I don't understand why they don't use money f...</td>\n",
       "      <td>Kinky-Monk ak32009 fools_eye None DwncstSheep...</td>\n",
       "      <td>Why does the government come with a begging bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mother's condition is going worse due to hepat...</td>\n",
       "      <td>756</td>\n",
       "      <td>g0zlly</td>\n",
       "      <td>Hi folks, I really appreciate the warm respons...</td>\n",
       "      <td>sanand_satwik</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g0zlly...</td>\n",
       "      <td>92</td>\n",
       "      <td>1.586871e+09</td>\n",
       "      <td>If anyone knows who is influential on Twitter...</td>\n",
       "      <td>AlternativeDrop6 TheRobotsHaveCome lanky32 pl...</td>\n",
       "      <td>Mother's condition is going worse due to hepat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0  Lost my Job, Sick Mother and Paralysed Dad, In...   1042  g014wc   \n",
       "1  Why does the government come with a begging bo...    647  fxofyu   \n",
       "2  Mother's condition is going worse due to hepat...    756  g0zlly   \n",
       "\n",
       "                                                body             author  \\\n",
       "0  Hi....It's really tough time for everyone. I r...      sanand_satwik   \n",
       "1  We have floods, terrorist attacks, famines due...  TWO-WHEELER-MAFIA   \n",
       "2  Hi folks, I really appreciate the warm respons...      sanand_satwik   \n",
       "\n",
       "      flair                                                url  comms_num  \\\n",
       "0  AskIndia  https://www.reddit.com/r/india/comments/g014wc...        132   \n",
       "1  AskIndia  https://www.reddit.com/r/india/comments/fxofyu...        205   \n",
       "2  AskIndia  https://www.reddit.com/r/india/comments/g0zlly...         92   \n",
       "\n",
       "        created                                            comment  \\\n",
       "0  1.586742e+09   I'm a freelancer. Don't listen to the idiots ...   \n",
       "1  1.586448e+09   I don't understand why they don't use money f...   \n",
       "2  1.586871e+09   If anyone knows who is influential on Twitter...   \n",
       "\n",
       "                                             authors  \\\n",
       "0   hashedram diabapp xataari Aashayrao sarcrasti...   \n",
       "1   Kinky-Monk ak32009 fools_eye None DwncstSheep...   \n",
       "2   AlternativeDrop6 TheRobotsHaveCome lanky32 pl...   \n",
       "\n",
       "                                   combined_features  \n",
       "0  Lost my Job, Sick Mother and Paralysed Dad, In...  \n",
       "1  Why does the government come with a begging bo...  \n",
       "2  Mother's condition is going worse due to hepat...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(PATH/'data/data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].astype(str)+df['body'].astype(str)+df['comment'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       AskIndia\n",
       "1       AskIndia\n",
       "2       AskIndia\n",
       "3       AskIndia\n",
       "4       AskIndia\n",
       "          ...   \n",
       "1795      Sports\n",
       "1796      Sports\n",
       "1797      Sports\n",
       "1798      Sports\n",
       "1799      Sports\n",
       "Name: flair, Length: 1800, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[['flair','title']]\n",
    "df[\"flair\"].apply(lambda x:str(x))\n",
    "# df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flairs=list(np.unique(df['flair']))\n",
    "df['flair']=df['flair'].apply(lambda x :all_flairs.index(x))\n",
    "all_flairs=list(np.unique(df['flair']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Lost my Job, Sick Mother and Paralysed Dad, In...\n",
       "1       Why does the government come with a begging bo...\n",
       "2       Mother's condition is going worse due to hepat...\n",
       "3       Men who are 30+ and have decided not to get ma...\n",
       "4       r/India: If money is no bar, would you prefer ...\n",
       "                              ...                        \n",
       "1795    Indian Women's and Men's Hockey Teams Seal Ber...\n",
       "1796    'It's humiliating for us': village disowns Dut...\n",
       "1797    Feroz Shah Kotla Renamed Arun Jaitley Stadium,...\n",
       "1798    Indian cricket fans are the most irritating ha...\n",
       "1799    Copy India's ambition to be the best: Ian Chap...\n",
       "Name: title, Length: 1800, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"].apply(lambda x:str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df['title'],df['flair'],random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNhRJZ5gskWb"
   },
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "fknRiEMExeog",
    "outputId": "fe96c224-4753-470d-d5fb-74646c635813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================   Naive Bayes =====================\n",
      "count_vectorizer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.89      0.38        44\n",
      "           1       0.62      0.23      0.33        44\n",
      "           2       0.92      0.47      0.62        51\n",
      "           3       0.17      0.15      0.16        52\n",
      "           4       0.93      0.25      0.40        51\n",
      "           5       0.38      0.53      0.44        49\n",
      "           6       0.48      0.76      0.59        55\n",
      "           7       0.56      0.10      0.17        51\n",
      "           8       1.00      0.38      0.55        53\n",
      "\n",
      "    accuracy                           0.42       450\n",
      "   macro avg       0.59      0.42      0.41       450\n",
      "weighted avg       0.60      0.42      0.41       450\n",
      "\n",
      "tfidf_vectorizer_word\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.89      0.33        44\n",
      "           1       0.61      0.32      0.42        44\n",
      "           2       0.91      0.41      0.57        51\n",
      "           3       0.43      0.17      0.25        52\n",
      "           4       0.94      0.63      0.75        51\n",
      "           5       0.46      0.37      0.41        49\n",
      "           6       0.53      0.80      0.64        55\n",
      "           7       0.78      0.14      0.23        51\n",
      "           8       1.00      0.51      0.67        53\n",
      "\n",
      "    accuracy                           0.47       450\n",
      "   macro avg       0.65      0.47      0.47       450\n",
      "weighted avg       0.66      0.47      0.48       450\n",
      "\n",
      "tfidf_vectorizer_word_ngram\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.82      0.37        44\n",
      "           1       0.52      0.27      0.36        44\n",
      "           2       0.41      0.14      0.21        51\n",
      "           3       0.31      0.17      0.22        52\n",
      "           4       0.67      0.51      0.58        51\n",
      "           5       0.44      0.47      0.46        49\n",
      "           6       0.37      0.73      0.49        55\n",
      "           7       0.21      0.06      0.09        51\n",
      "           8       0.75      0.23      0.35        53\n",
      "\n",
      "    accuracy                           0.37       450\n",
      "   macro avg       0.44      0.38      0.35       450\n",
      "weighted avg       0.44      0.37      0.35       450\n",
      "\n",
      "tfidf_vectorizer_ngram_chars\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.93      0.26        44\n",
      "           1       0.71      0.23      0.34        44\n",
      "           2       0.90      0.18      0.30        51\n",
      "           3       0.25      0.08      0.12        52\n",
      "           4       0.93      0.25      0.40        51\n",
      "           5       0.46      0.27      0.34        49\n",
      "           6       0.43      0.51      0.47        55\n",
      "           7       0.50      0.08      0.14        51\n",
      "           8       0.95      0.38      0.54        53\n",
      "\n",
      "    accuracy                           0.32       450\n",
      "   macro avg       0.59      0.32      0.32       450\n",
      "weighted avg       0.59      0.32      0.32       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===========================   Naive Bayes =====================')\n",
    "\n",
    "\n",
    "vectorizers=[(count_vect,'count_vectorizer'),(tfidf_vect,'tfidf_vectorizer_word'),(tfidf_vect_ngram,'tfidf_vectorizer_word_ngram'),(tfidf_vect_ngram_chars,'tfidf_vectorizer_ngram_chars')]\n",
    "\n",
    "#Naive Bayes for all features:\n",
    "for vectorizer in vectorizers:\n",
    "  print(vectorizer[1])\n",
    "  clf = Pipeline([\n",
    "    ('vect',vectorizer[0]),\n",
    "    ('clf', MultinomialNB()),\n",
    "  ])\n",
    "  clf = clf.fit(X_train, y_train)\n",
    "  predictions=clf.predict_proba(X_test)\n",
    "  test_preds=np.argmax(predictions,axis=1)\n",
    "\n",
    "  auc = classification_report(y_test, test_preds)\n",
    "  print (auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "GgUAwX9RxqIu",
    "outputId": "4978d097-2f4b-4f33-dda3-ce8cac854ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================   Logistic Regression =====================\n",
      "count_vectorizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/ml/midas_internship/env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.45      0.53        44\n",
      "           1       0.45      0.43      0.44        44\n",
      "           2       0.74      0.57      0.64        51\n",
      "           3       0.41      0.42      0.42        52\n",
      "           4       0.76      0.80      0.78        51\n",
      "           5       0.44      0.45      0.44        49\n",
      "           6       0.63      0.62      0.62        55\n",
      "           7       0.49      0.53      0.51        51\n",
      "           8       0.66      0.87      0.75        53\n",
      "\n",
      "    accuracy                           0.58       450\n",
      "   macro avg       0.58      0.57      0.57       450\n",
      "weighted avg       0.58      0.58      0.57       450\n",
      "\n",
      "tfidf_vectorizer_word\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.59      0.57        44\n",
      "           1       0.55      0.50      0.52        44\n",
      "           2       0.79      0.67      0.72        51\n",
      "           3       0.50      0.44      0.47        52\n",
      "           4       0.82      0.80      0.81        51\n",
      "           5       0.49      0.47      0.48        49\n",
      "           6       0.62      0.82      0.71        55\n",
      "           7       0.51      0.47      0.49        51\n",
      "           8       0.79      0.87      0.83        53\n",
      "\n",
      "    accuracy                           0.63       450\n",
      "   macro avg       0.63      0.63      0.62       450\n",
      "weighted avg       0.63      0.63      0.63       450\n",
      "\n",
      "tfidf_vectorizer_word_ngram\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.64      0.47        44\n",
      "           1       0.55      0.41      0.47        44\n",
      "           2       0.43      0.25      0.32        51\n",
      "           3       0.34      0.19      0.25        52\n",
      "           4       0.57      0.63      0.60        51\n",
      "           5       0.44      0.45      0.44        49\n",
      "           6       0.45      0.64      0.53        55\n",
      "           7       0.29      0.22      0.25        51\n",
      "           8       0.56      0.66      0.60        53\n",
      "\n",
      "    accuracy                           0.45       450\n",
      "   macro avg       0.45      0.45      0.44       450\n",
      "weighted avg       0.45      0.45      0.44       450\n",
      "\n",
      "tfidf_vectorizer_ngram_chars\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.70      0.61        44\n",
      "           1       0.50      0.41      0.45        44\n",
      "           2       0.78      0.57      0.66        51\n",
      "           3       0.62      0.44      0.52        52\n",
      "           4       0.67      0.80      0.73        51\n",
      "           5       0.45      0.41      0.43        49\n",
      "           6       0.51      0.80      0.62        55\n",
      "           7       0.68      0.51      0.58        51\n",
      "           8       0.79      0.79      0.79        53\n",
      "\n",
      "    accuracy                           0.61       450\n",
      "   macro avg       0.62      0.60      0.60       450\n",
      "weighted avg       0.62      0.61      0.60       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===========================   Logistic Regression =====================')\n",
    "\n",
    "\n",
    "vectorizers=[(count_vect,'count_vectorizer'),(tfidf_vect,'tfidf_vectorizer_word'),(tfidf_vect_ngram,'tfidf_vectorizer_word_ngram'),(tfidf_vect_ngram_chars,'tfidf_vectorizer_ngram_chars')]\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "  print(vectorizer[1])\n",
    "  clf = Pipeline([\n",
    "    ('vect',vectorizer[0]),\n",
    "    ('clf', linear_model.LogisticRegression(multi_class='auto',solver='lbfgs')),\n",
    "  ])\n",
    "  clf = clf.fit(X_train, y_train)\n",
    "  predictions=clf.predict_proba(X_test)\n",
    "  test_preds=np.argmax(predictions,axis=1)\n",
    "\n",
    "  auc = classification_report(y_test, test_preds)\n",
    "  print (auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "ZKIBVfe9xuxF",
    "outputId": "2cf1176a-f580-4511-c544-ecec63cc76ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================   SVM =====================\n",
      "count_vectorizer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.68      0.49        44\n",
      "           1       0.20      0.20      0.20        44\n",
      "           2       0.35      0.24      0.28        51\n",
      "           3       0.32      0.17      0.23        52\n",
      "           4       0.50      0.61      0.55        51\n",
      "           5       0.32      0.24      0.28        49\n",
      "           6       0.47      0.65      0.55        55\n",
      "           7       0.30      0.06      0.10        51\n",
      "           8       0.44      0.66      0.53        53\n",
      "\n",
      "    accuracy                           0.39       450\n",
      "   macro avg       0.37      0.39      0.36       450\n",
      "weighted avg       0.37      0.39      0.36       450\n",
      "\n",
      "tfidf_vectorizer_word\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.55      0.53        44\n",
      "           1       0.50      0.43      0.46        44\n",
      "           2       0.83      0.59      0.69        51\n",
      "           3       0.47      0.52      0.49        52\n",
      "           4       0.83      0.76      0.80        51\n",
      "           5       0.48      0.43      0.45        49\n",
      "           6       0.61      0.78      0.68        55\n",
      "           7       0.53      0.57      0.55        51\n",
      "           8       0.80      0.83      0.81        53\n",
      "\n",
      "    accuracy                           0.61       450\n",
      "   macro avg       0.62      0.61      0.61       450\n",
      "weighted avg       0.62      0.61      0.61       450\n",
      "\n",
      "tfidf_vectorizer_word_ngram\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.59      0.48        44\n",
      "           1       0.48      0.45      0.47        44\n",
      "           2       0.44      0.33      0.38        51\n",
      "           3       0.35      0.29      0.32        52\n",
      "           4       0.57      0.57      0.57        51\n",
      "           5       0.42      0.39      0.40        49\n",
      "           6       0.46      0.65      0.54        55\n",
      "           7       0.33      0.24      0.28        51\n",
      "           8       0.59      0.57      0.58        53\n",
      "\n",
      "    accuracy                           0.45       450\n",
      "   macro avg       0.45      0.45      0.44       450\n",
      "weighted avg       0.45      0.45      0.45       450\n",
      "\n",
      "tfidf_vectorizer_ngram_chars\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.61      0.56        44\n",
      "           1       0.44      0.39      0.41        44\n",
      "           2       0.71      0.57      0.63        51\n",
      "           3       0.55      0.46      0.50        52\n",
      "           4       0.71      0.78      0.75        51\n",
      "           5       0.43      0.41      0.42        49\n",
      "           6       0.56      0.75      0.64        55\n",
      "           7       0.54      0.51      0.53        51\n",
      "           8       0.86      0.79      0.82        53\n",
      "\n",
      "    accuracy                           0.59       450\n",
      "   macro avg       0.59      0.59      0.58       450\n",
      "weighted avg       0.59      0.59      0.59       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===========================   SVM =====================')\n",
    "\n",
    "\n",
    "vectorizers=[(count_vect,'count_vectorizer'),(tfidf_vect,'tfidf_vectorizer_word'),(tfidf_vect_ngram,'tfidf_vectorizer_word_ngram'),(tfidf_vect_ngram_chars,'tfidf_vectorizer_ngram_chars')]\n",
    "\n",
    "# SVM on count vectors: SVM Classifier Pipeline on word count vector\n",
    "for vectorizer in vectorizers:\n",
    "  print(vectorizer[1])\n",
    "  clf = Pipeline([\n",
    "    ('vect',vectorizer[0]),\n",
    "    ('clf', SVC(gamma='scale',probability=True)),\n",
    "  ])\n",
    "  clf = clf.fit(X_train, y_train)\n",
    "  predictions=clf.predict_proba(X_test)\n",
    "  test_preds=np.argmax(predictions,axis=1)\n",
    "\n",
    "  auc = classification_report(y_test, test_preds)\n",
    "  print (auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jze-dR6PuBT6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "baselines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
