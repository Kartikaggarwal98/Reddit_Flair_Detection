{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AM0k3vFh9Psd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zfR-LKtx9Psi",
    "outputId": "1a88c4ca-e9f7-4b09-d317-ea5b2c0b5180"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline,linear_model\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "EcnzDbcy9Psl",
    "outputId": "0e213316-d2d2-4572-d1d6-f84fdf6e187b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from csv\n",
    "The dataset consists of details of 200 posts for each flair in r/india. The data was collected on 15 April,2020. \n",
    "\n",
    "Each posts has 10 comments and the flairs used are 'AskIndia', 'Business/Finance', 'Food', 'Non-Political', 'Photography', 'Policy/Economy', 'Politics', 'Science/Technology','Sports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzPFW3np9ii4"
   },
   "outputs": [],
   "source": [
    "PATH=Path(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjZRmbWVASRC",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>flair</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>comment</th>\n",
       "      <th>authors</th>\n",
       "      <th>combined_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lost my Job, Sick Mother and Paralysed Dad, In...</td>\n",
       "      <td>1042</td>\n",
       "      <td>g014wc</td>\n",
       "      <td>Hi....It's really tough time for everyone. I r...</td>\n",
       "      <td>sanand_satwik</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g014wc...</td>\n",
       "      <td>132</td>\n",
       "      <td>1.586742e+09</td>\n",
       "      <td>I'm a freelancer. Don't listen to the idiots ...</td>\n",
       "      <td>hashedram diabapp xataari Aashayrao sarcrasti...</td>\n",
       "      <td>Lost my Job, Sick Mother and Paralysed Dad, In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why does the government come with a begging bo...</td>\n",
       "      <td>647</td>\n",
       "      <td>fxofyu</td>\n",
       "      <td>We have floods, terrorist attacks, famines due...</td>\n",
       "      <td>TWO-WHEELER-MAFIA</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fxofyu...</td>\n",
       "      <td>205</td>\n",
       "      <td>1.586448e+09</td>\n",
       "      <td>I don't understand why they don't use money f...</td>\n",
       "      <td>Kinky-Monk ak32009 fools_eye None DwncstSheep...</td>\n",
       "      <td>Why does the government come with a begging bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mother's condition is going worse due to hepat...</td>\n",
       "      <td>756</td>\n",
       "      <td>g0zlly</td>\n",
       "      <td>Hi folks, I really appreciate the warm respons...</td>\n",
       "      <td>sanand_satwik</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g0zlly...</td>\n",
       "      <td>92</td>\n",
       "      <td>1.586871e+09</td>\n",
       "      <td>If anyone knows who is influential on Twitter...</td>\n",
       "      <td>AlternativeDrop6 TheRobotsHaveCome lanky32 pl...</td>\n",
       "      <td>Mother's condition is going worse due to hepat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0  Lost my Job, Sick Mother and Paralysed Dad, In...   1042  g014wc   \n",
       "1  Why does the government come with a begging bo...    647  fxofyu   \n",
       "2  Mother's condition is going worse due to hepat...    756  g0zlly   \n",
       "\n",
       "                                                body             author  \\\n",
       "0  Hi....It's really tough time for everyone. I r...      sanand_satwik   \n",
       "1  We have floods, terrorist attacks, famines due...  TWO-WHEELER-MAFIA   \n",
       "2  Hi folks, I really appreciate the warm respons...      sanand_satwik   \n",
       "\n",
       "      flair                                                url  comms_num  \\\n",
       "0  AskIndia  https://www.reddit.com/r/india/comments/g014wc...        132   \n",
       "1  AskIndia  https://www.reddit.com/r/india/comments/fxofyu...        205   \n",
       "2  AskIndia  https://www.reddit.com/r/india/comments/g0zlly...         92   \n",
       "\n",
       "        created                                            comment  \\\n",
       "0  1.586742e+09   I'm a freelancer. Don't listen to the idiots ...   \n",
       "1  1.586448e+09   I don't understand why they don't use money f...   \n",
       "2  1.586871e+09   If anyone knows who is influential on Twitter...   \n",
       "\n",
       "                                             authors  \\\n",
       "0   hashedram diabapp xataari Aashayrao sarcrasti...   \n",
       "1   Kinky-Monk ak32009 fools_eye None DwncstSheep...   \n",
       "2   AlternativeDrop6 TheRobotsHaveCome lanky32 pl...   \n",
       "\n",
       "                                   combined_features  \n",
       "0  Lost my Job, Sick Mother and Paralysed Dad, In...  \n",
       "1  Why does the government come with a begging bo...  \n",
       "2  Mother's condition is going worse due to hepat...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(PATH/'data/data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using combination of title, body text and the comments as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using only relevant columns\n",
    "df=df[['flair','combined_features']]\n",
    "# df['title'] = df['title'].astype(str)+df['body'].astype(str)\n",
    "\n",
    "df[\"flair\"].apply(lambda x:str(x))\n",
    "df[\"combined_features\"].apply(lambda x:str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of flairs in the dataset and convert into integers labels\n",
    "all_flairs=list(np.unique(df['flair']))\n",
    "df['flair']=df['flair'].apply(lambda x :all_flairs.index(x))\n",
    "all_flairs=list(np.unique(df['flair']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning\n",
    "Two cleaning methods are used:\n",
    "1. Stemming\n",
    "2. Without stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data using two methods.\n",
    "def preprocess_stem(x):\n",
    "    stemmer = PorterStemmer()\n",
    "    x = x.lower()\n",
    "    x = re.sub(\"[^a-z0-9'@+-/]\",' ',x)\n",
    "    tokens = x.split()\n",
    "    new_tok = [i for i in tokens if i not in Stopwords]\n",
    "    return ' '.join([stemmer.stem(i) for i in new_tok])\n",
    "\n",
    "def preprocess(x):\n",
    "    x = x.lower()\n",
    "    x = re.sub(\"[^a-z0-9'@+-/]\",' ',x)\n",
    "    tokens = x.split()\n",
    "    new_tok = [i for i in tokens if i not in Stopwords]\n",
    "    return ' '.join(new_tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing can be skipped as the results were nearly similar after and before cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stopwords = list(stopwords.words('english'))\n",
    "df[\"title\"].apply(lambda x:preprocess_stem(x))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df['title'],df['flair'],random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text data into vectors\n",
    "\n",
    "4 different methods are used to vectorize the words:\n",
    "1. Count based Vectorization\n",
    "2. Word level Term-frequency inverse document frequency (tf-idf)\n",
    "3. n-gram level tfidf\n",
    "4. character level tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNhRJZ5gskWb"
   },
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Models\n",
    "Various classification models are used for classification: Linear SVM, Random Forest, Naive Bayes, Logistic Regression, SVM.\n",
    "\n",
    "The scores are reported on validation set for all 4 vectorization methods and the best one is selected to report the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================   Linear SVM =====================\n",
      "count_vectorizer\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          AskIndia       0.37      0.57      0.45        44\n",
      "  Business/Finance       0.26      0.59      0.36        44\n",
      "              Food       0.67      0.31      0.43        51\n",
      "     Non-Political       0.36      0.23      0.28        52\n",
      "       Photography       0.61      0.84      0.70        51\n",
      "    Policy/Economy       0.39      0.37      0.38        49\n",
      "          Politics       0.58      0.40      0.47        55\n",
      "Science/Technology       0.35      0.24      0.28        51\n",
      "            Sports       0.86      0.57      0.68        53\n",
      "\n",
      "          accuracy                           0.45       450\n",
      "         macro avg       0.49      0.46      0.45       450\n",
      "      weighted avg       0.50      0.45      0.45       450\n",
      "\n",
      "tfidf_vectorizer_word\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          AskIndia       0.52      0.59      0.55        44\n",
      "  Business/Finance       0.47      0.43      0.45        44\n",
      "              Food       0.82      0.78      0.80        51\n",
      "     Non-Political       0.55      0.35      0.42        52\n",
      "       Photography       0.76      0.92      0.83        51\n",
      "    Policy/Economy       0.45      0.53      0.49        49\n",
      "          Politics       0.71      0.75      0.73        55\n",
      "Science/Technology       0.51      0.39      0.44        51\n",
      "            Sports       0.80      0.92      0.86        53\n",
      "\n",
      "          accuracy                           0.64       450\n",
      "         macro avg       0.62      0.63      0.62       450\n",
      "      weighted avg       0.63      0.64      0.63       450\n",
      "\n",
      "tfidf_vectorizer_word_ngram\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          AskIndia       0.42      0.61      0.50        44\n",
      "  Business/Finance       0.46      0.41      0.43        44\n",
      "              Food       0.39      0.31      0.35        51\n",
      "     Non-Political       0.36      0.23      0.28        52\n",
      "       Photography       0.54      0.69      0.60        51\n",
      "    Policy/Economy       0.37      0.41      0.39        49\n",
      "          Politics       0.47      0.51      0.49        55\n",
      "Science/Technology       0.29      0.24      0.26        51\n",
      "            Sports       0.62      0.60      0.61        53\n",
      "\n",
      "          accuracy                           0.44       450\n",
      "         macro avg       0.43      0.45      0.43       450\n",
      "      weighted avg       0.44      0.44      0.43       450\n",
      "\n",
      "tfidf_vectorizer_ngram_chars\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          AskIndia       0.53      0.66      0.59        44\n",
      "  Business/Finance       0.37      0.43      0.40        44\n",
      "              Food       0.77      0.73      0.75        51\n",
      "     Non-Political       0.80      0.15      0.26        52\n",
      "       Photography       0.79      0.82      0.81        51\n",
      "    Policy/Economy       0.41      0.47      0.44        49\n",
      "          Politics       0.58      0.69      0.63        55\n",
      "Science/Technology       0.53      0.53      0.53        51\n",
      "            Sports       0.81      0.91      0.86        53\n",
      "\n",
      "          accuracy                           0.60       450\n",
      "         macro avg       0.62      0.60      0.58       450\n",
      "      weighted avg       0.63      0.60      0.59       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===========================   Linear SVM =====================')\n",
    "\n",
    "\n",
    "vectorizers=[(count_vect,'count_vectorizer'),(tfidf_vect,'tfidf_vectorizer_word'),(tfidf_vect_ngram,'tfidf_vectorizer_word_ngram'),(tfidf_vect_ngram_chars,'tfidf_vectorizer_ngram_chars')]\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    print(vectorizer[1])\n",
    "    clf = Pipeline([\n",
    "    ('vect',vectorizer[0]),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=4, tol=None))\n",
    "    ])\n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    test_preds=clf.predict(X_test)\n",
    "\n",
    "    auc = classification_report(y_test, test_preds)\n",
    "    print (auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================   Naive Bayes =====================\n",
      "count_vectorizer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.82      0.50        44\n",
      "           1       0.45      0.39      0.41        44\n",
      "           2       0.33      0.20      0.25        51\n",
      "           3       0.20      0.25      0.22        52\n",
      "           4       0.71      0.69      0.70        51\n",
      "           5       0.44      0.37      0.40        49\n",
      "           6       0.46      0.47      0.47        55\n",
      "           7       0.25      0.12      0.16        51\n",
      "           8       0.70      0.62      0.66        53\n",
      "\n",
      "    accuracy                           0.43       450\n",
      "   macro avg       0.43      0.44      0.42       450\n",
      "weighted avg       0.44      0.43      0.42       450\n",
      "\n",
      "tfidf_vectorizer_word\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.84      0.54        44\n",
      "           1       0.48      0.36      0.42        44\n",
      "           2       0.39      0.27      0.32        51\n",
      "           3       0.15      0.15      0.15        52\n",
      "           4       0.54      0.76      0.63        51\n",
      "           5       0.35      0.33      0.34        49\n",
      "           6       0.54      0.36      0.43        55\n",
      "           7       0.29      0.20      0.24        51\n",
      "           8       0.57      0.51      0.54        53\n",
      "\n",
      "    accuracy                           0.42       450\n",
      "   macro avg       0.41      0.42      0.40       450\n",
      "weighted avg       0.41      0.42      0.40       450\n",
      "\n",
      "tfidf_vectorizer_word_ngram\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.55      0.46        44\n",
      "           1       0.43      0.34      0.38        44\n",
      "           2       0.25      0.16      0.19        51\n",
      "           3       0.04      0.02      0.03        52\n",
      "           4       0.19      0.59      0.28        51\n",
      "           5       0.36      0.18      0.24        49\n",
      "           6       0.37      0.20      0.26        55\n",
      "           7       0.22      0.16      0.18        51\n",
      "           8       0.39      0.32      0.35        53\n",
      "\n",
      "    accuracy                           0.27       450\n",
      "   macro avg       0.29      0.28      0.26       450\n",
      "weighted avg       0.29      0.27      0.26       450\n",
      "\n",
      "tfidf_vectorizer_ngram_chars\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.77      0.48        44\n",
      "           1       0.40      0.39      0.39        44\n",
      "           2       0.34      0.22      0.27        51\n",
      "           3       0.30      0.17      0.22        52\n",
      "           4       0.63      0.73      0.67        51\n",
      "           5       0.38      0.37      0.37        49\n",
      "           6       0.44      0.38      0.41        55\n",
      "           7       0.25      0.14      0.18        51\n",
      "           8       0.45      0.57      0.50        53\n",
      "\n",
      "    accuracy                           0.41       450\n",
      "   macro avg       0.39      0.41      0.39       450\n",
      "weighted avg       0.39      0.41      0.39       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===========================   Random Forest =====================')\n",
    "\n",
    "\n",
    "vectorizers=[(count_vect,'count_vectorizer'),(tfidf_vect,'tfidf_vectorizer_word'),(tfidf_vect_ngram,'tfidf_vectorizer_word_ngram'),(tfidf_vect_ngram_chars,'tfidf_vectorizer_ngram_chars')]\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    print(vectorizer[1])\n",
    "    clf = Pipeline([\n",
    "    ('vect',vectorizer[0]),\n",
    "    ('clf', RandomForestClassifier(n_estimators = 80, random_state = 42)),\n",
    "    ])\n",
    "    \n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    predictions=clf.predict_proba(X_test)\n",
    "    test_preds=np.argmax(predictions,axis=1)\n",
    "\n",
    "    auc = classification_report(y_test, test_preds)\n",
    "    print (auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "fknRiEMExeog",
    "outputId": "fe96c224-4753-470d-d5fb-74646c635813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================   Naive Bayes =====================\n",
      "count_vectorizer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.89      0.38        44\n",
      "           1       0.62      0.23      0.33        44\n",
      "           2       0.92      0.47      0.62        51\n",
      "           3       0.17      0.15      0.16        52\n",
      "           4       0.93      0.25      0.40        51\n",
      "           5       0.38      0.53      0.44        49\n",
      "           6       0.48      0.76      0.59        55\n",
      "           7       0.56      0.10      0.17        51\n",
      "           8       1.00      0.38      0.55        53\n",
      "\n",
      "    accuracy                           0.42       450\n",
      "   macro avg       0.59      0.42      0.41       450\n",
      "weighted avg       0.60      0.42      0.41       450\n",
      "\n",
      "tfidf_vectorizer_word\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.89      0.33        44\n",
      "           1       0.61      0.32      0.42        44\n",
      "           2       0.91      0.41      0.57        51\n",
      "           3       0.43      0.17      0.25        52\n",
      "           4       0.94      0.63      0.75        51\n",
      "           5       0.46      0.37      0.41        49\n",
      "           6       0.53      0.80      0.64        55\n",
      "           7       0.78      0.14      0.23        51\n",
      "           8       1.00      0.51      0.67        53\n",
      "\n",
      "    accuracy                           0.47       450\n",
      "   macro avg       0.65      0.47      0.47       450\n",
      "weighted avg       0.66      0.47      0.48       450\n",
      "\n",
      "tfidf_vectorizer_word_ngram\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.82      0.37        44\n",
      "           1       0.52      0.27      0.36        44\n",
      "           2       0.41      0.14      0.21        51\n",
      "           3       0.31      0.17      0.22        52\n",
      "           4       0.67      0.51      0.58        51\n",
      "           5       0.44      0.47      0.46        49\n",
      "           6       0.37      0.73      0.49        55\n",
      "           7       0.21      0.06      0.09        51\n",
      "           8       0.75      0.23      0.35        53\n",
      "\n",
      "    accuracy                           0.37       450\n",
      "   macro avg       0.44      0.38      0.35       450\n",
      "weighted avg       0.44      0.37      0.35       450\n",
      "\n",
      "tfidf_vectorizer_ngram_chars\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.93      0.26        44\n",
      "           1       0.71      0.23      0.34        44\n",
      "           2       0.90      0.18      0.30        51\n",
      "           3       0.25      0.08      0.12        52\n",
      "           4       0.93      0.25      0.40        51\n",
      "           5       0.46      0.27      0.34        49\n",
      "           6       0.43      0.51      0.47        55\n",
      "           7       0.50      0.08      0.14        51\n",
      "           8       0.95      0.38      0.54        53\n",
      "\n",
      "    accuracy                           0.32       450\n",
      "   macro avg       0.59      0.32      0.32       450\n",
      "weighted avg       0.59      0.32      0.32       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===========================   Naive Bayes =====================')\n",
    "\n",
    "\n",
    "vectorizers=[(count_vect,'count_vectorizer'),(tfidf_vect,'tfidf_vectorizer_word'),(tfidf_vect_ngram,'tfidf_vectorizer_word_ngram'),(tfidf_vect_ngram_chars,'tfidf_vectorizer_ngram_chars')]\n",
    "\n",
    "#Naive Bayes for all features:\n",
    "for vectorizer in vectorizers:\n",
    "    print(vectorizer[1])\n",
    "    clf = Pipeline([\n",
    "    ('vect',vectorizer[0]),\n",
    "    ('clf', MultinomialNB()),\n",
    "    ])\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    predictions=clf.predict_proba(X_test)\n",
    "    test_preds=np.argmax(predictions,axis=1)\n",
    "\n",
    "    auc = classification_report(y_test, test_preds)\n",
    "    print (auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "GgUAwX9RxqIu",
    "outputId": "4978d097-2f4b-4f33-dda3-ce8cac854ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================   Logistic Regression =====================\n",
      "count_vectorizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/ml/midas_internship/env/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.45      0.53        44\n",
      "           1       0.45      0.43      0.44        44\n",
      "           2       0.74      0.57      0.64        51\n",
      "           3       0.41      0.42      0.42        52\n",
      "           4       0.76      0.80      0.78        51\n",
      "           5       0.44      0.45      0.44        49\n",
      "           6       0.63      0.62      0.62        55\n",
      "           7       0.49      0.53      0.51        51\n",
      "           8       0.66      0.87      0.75        53\n",
      "\n",
      "    accuracy                           0.58       450\n",
      "   macro avg       0.58      0.57      0.57       450\n",
      "weighted avg       0.58      0.58      0.57       450\n",
      "\n",
      "tfidf_vectorizer_word\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.59      0.57        44\n",
      "           1       0.55      0.50      0.52        44\n",
      "           2       0.79      0.67      0.72        51\n",
      "           3       0.50      0.44      0.47        52\n",
      "           4       0.82      0.80      0.81        51\n",
      "           5       0.49      0.47      0.48        49\n",
      "           6       0.62      0.82      0.71        55\n",
      "           7       0.51      0.47      0.49        51\n",
      "           8       0.79      0.87      0.83        53\n",
      "\n",
      "    accuracy                           0.63       450\n",
      "   macro avg       0.63      0.63      0.62       450\n",
      "weighted avg       0.63      0.63      0.63       450\n",
      "\n",
      "tfidf_vectorizer_word_ngram\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.64      0.47        44\n",
      "           1       0.55      0.41      0.47        44\n",
      "           2       0.43      0.25      0.32        51\n",
      "           3       0.34      0.19      0.25        52\n",
      "           4       0.57      0.63      0.60        51\n",
      "           5       0.44      0.45      0.44        49\n",
      "           6       0.45      0.64      0.53        55\n",
      "           7       0.29      0.22      0.25        51\n",
      "           8       0.56      0.66      0.60        53\n",
      "\n",
      "    accuracy                           0.45       450\n",
      "   macro avg       0.45      0.45      0.44       450\n",
      "weighted avg       0.45      0.45      0.44       450\n",
      "\n",
      "tfidf_vectorizer_ngram_chars\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.70      0.61        44\n",
      "           1       0.50      0.41      0.45        44\n",
      "           2       0.78      0.57      0.66        51\n",
      "           3       0.62      0.44      0.52        52\n",
      "           4       0.67      0.80      0.73        51\n",
      "           5       0.45      0.41      0.43        49\n",
      "           6       0.51      0.80      0.62        55\n",
      "           7       0.68      0.51      0.58        51\n",
      "           8       0.79      0.79      0.79        53\n",
      "\n",
      "    accuracy                           0.61       450\n",
      "   macro avg       0.62      0.60      0.60       450\n",
      "weighted avg       0.62      0.61      0.60       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===========================   Logistic Regression =====================')\n",
    "\n",
    "vectorizers=[(count_vect,'count_vectorizer'),(tfidf_vect,'tfidf_vectorizer_word'),(tfidf_vect_ngram,'tfidf_vectorizer_word_ngram'),(tfidf_vect_ngram_chars,'tfidf_vectorizer_ngram_chars')]\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    print(vectorizer[1])\n",
    "    clf = Pipeline([\n",
    "    ('vect',vectorizer[0]),\n",
    "    ('clf', linear_model.LogisticRegression(multi_class='auto',solver='lbfgs')),\n",
    "    ])\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    predictions=clf.predict_proba(X_test)\n",
    "    test_preds=np.argmax(predictions,axis=1)\n",
    "\n",
    "    auc = classification_report(y_test, test_preds)\n",
    "    print (auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "ZKIBVfe9xuxF",
    "outputId": "2cf1176a-f580-4511-c544-ecec63cc76ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================   SVM =====================\n",
      "count_vectorizer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.68      0.50        44\n",
      "           1       0.25      0.20      0.23        44\n",
      "           2       0.33      0.24      0.28        51\n",
      "           3       0.32      0.17      0.23        52\n",
      "           4       0.56      0.59      0.57        51\n",
      "           5       0.32      0.20      0.25        49\n",
      "           6       0.46      0.65      0.54        55\n",
      "           7       0.27      0.12      0.16        51\n",
      "           8       0.41      0.68      0.51        53\n",
      "\n",
      "    accuracy                           0.40       450\n",
      "   macro avg       0.37      0.39      0.36       450\n",
      "weighted avg       0.37      0.40      0.36       450\n",
      "\n",
      "tfidf_vectorizer_word\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.55      0.53        44\n",
      "           1       0.50      0.43      0.46        44\n",
      "           2       0.83      0.59      0.69        51\n",
      "           3       0.48      0.56      0.52        52\n",
      "           4       0.83      0.78      0.81        51\n",
      "           5       0.48      0.43      0.45        49\n",
      "           6       0.62      0.78      0.69        55\n",
      "           7       0.53      0.57      0.55        51\n",
      "           8       0.81      0.83      0.82        53\n",
      "\n",
      "    accuracy                           0.62       450\n",
      "   macro avg       0.62      0.61      0.61       450\n",
      "weighted avg       0.63      0.62      0.62       450\n",
      "\n",
      "tfidf_vectorizer_word_ngram\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.59      0.48        44\n",
      "           1       0.46      0.43      0.45        44\n",
      "           2       0.44      0.33      0.38        51\n",
      "           3       0.33      0.29      0.31        52\n",
      "           4       0.57      0.57      0.57        51\n",
      "           5       0.41      0.37      0.39        49\n",
      "           6       0.46      0.65      0.54        55\n",
      "           7       0.33      0.24      0.28        51\n",
      "           8       0.59      0.57      0.58        53\n",
      "\n",
      "    accuracy                           0.45       450\n",
      "   macro avg       0.44      0.45      0.44       450\n",
      "weighted avg       0.44      0.45      0.44       450\n",
      "\n",
      "tfidf_vectorizer_ngram_chars\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.61      0.56        44\n",
      "           1       0.44      0.39      0.41        44\n",
      "           2       0.71      0.59      0.65        51\n",
      "           3       0.55      0.44      0.49        52\n",
      "           4       0.74      0.78      0.76        51\n",
      "           5       0.43      0.41      0.42        49\n",
      "           6       0.55      0.75      0.63        55\n",
      "           7       0.53      0.51      0.52        51\n",
      "           8       0.82      0.79      0.81        53\n",
      "\n",
      "    accuracy                           0.59       450\n",
      "   macro avg       0.59      0.59      0.58       450\n",
      "weighted avg       0.59      0.59      0.59       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('===========================   SVM =====================')\n",
    "\n",
    "\n",
    "vectorizers=[(count_vect,'count_vectorizer'),(tfidf_vect,'tfidf_vectorizer_word'),(tfidf_vect_ngram,'tfidf_vectorizer_word_ngram'),(tfidf_vect_ngram_chars,'tfidf_vectorizer_ngram_chars')]\n",
    "\n",
    "# SVM on count vectors: SVM Classifier Pipeline on word count vector\n",
    "for vectorizer in vectorizers:\n",
    "    print(vectorizer[1])\n",
    "    clf = Pipeline([\n",
    "    ('vect',vectorizer[0]),\n",
    "    ('clf', SVC(gamma='scale',probability=True)),\n",
    "    ])\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    predictions=clf.predict_proba(X_test)\n",
    "    test_preds=np.argmax(predictions,axis=1)\n",
    "    auc = classification_report(y_test, test_preds)\n",
    "    print (auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The logistic regression model performs the best among all models. This model is saved using pickle module and used for  final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "clf = Pipeline([\n",
    "    ('vect',tfidf_vect),\n",
    "    ('clf', linear_model.LogisticRegression(multi_class='auto',solver='lbfgs')),\n",
    "  ])\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "filename = 'logreg_tfidf.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "baselines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
