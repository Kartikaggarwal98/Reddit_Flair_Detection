{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W66sxaAa9SsV"
   },
   "outputs": [],
   "source": [
    "! pip install spacy\n",
    "! python -m spacy download en\n",
    "! pip install torchtext==0.2.3\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AM0k3vFh9Psd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrNRTYpt9dsT"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# from fastai.text import *\n",
    "# import preprocessor as pre\n",
    "# pre.set_options(pre.OPT.URL,pre.OPT.NUMBER,pre.OPT.MENTION,pre.OPT.HASHTAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zfR-LKtx9Psi",
    "outputId": "1a88c4ca-e9f7-4b09-d317-ea5b2c0b5180"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline,linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from fastai.plots import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "EcnzDbcy9Psl",
    "outputId": "0e213316-d2d2-4572-d1d6-f84fdf6e187b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzPFW3np9ii4"
   },
   "outputs": [],
   "source": [
    "PATH=Path(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qjZRmbWVASRC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>author</th>\n",
       "      <th>flair</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>comment</th>\n",
       "      <th>authors</th>\n",
       "      <th>combined_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lost my Job, Sick Mother and Paralysed Dad, In...</td>\n",
       "      <td>1042</td>\n",
       "      <td>g014wc</td>\n",
       "      <td>Hi....It's really tough time for everyone. I r...</td>\n",
       "      <td>sanand_satwik</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g014wc...</td>\n",
       "      <td>132</td>\n",
       "      <td>1.586742e+09</td>\n",
       "      <td>I'm a freelancer. Don't listen to the idiots ...</td>\n",
       "      <td>hashedram diabapp xataari Aashayrao sarcrasti...</td>\n",
       "      <td>Lost my Job, Sick Mother and Paralysed Dad, In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why does the government come with a begging bo...</td>\n",
       "      <td>647</td>\n",
       "      <td>fxofyu</td>\n",
       "      <td>We have floods, terrorist attacks, famines due...</td>\n",
       "      <td>TWO-WHEELER-MAFIA</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/fxofyu...</td>\n",
       "      <td>205</td>\n",
       "      <td>1.586448e+09</td>\n",
       "      <td>I don't understand why they don't use money f...</td>\n",
       "      <td>Kinky-Monk ak32009 fools_eye None DwncstSheep...</td>\n",
       "      <td>Why does the government come with a begging bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mother's condition is going worse due to hepat...</td>\n",
       "      <td>756</td>\n",
       "      <td>g0zlly</td>\n",
       "      <td>Hi folks, I really appreciate the warm respons...</td>\n",
       "      <td>sanand_satwik</td>\n",
       "      <td>AskIndia</td>\n",
       "      <td>https://www.reddit.com/r/india/comments/g0zlly...</td>\n",
       "      <td>92</td>\n",
       "      <td>1.586871e+09</td>\n",
       "      <td>If anyone knows who is influential on Twitter...</td>\n",
       "      <td>AlternativeDrop6 TheRobotsHaveCome lanky32 pl...</td>\n",
       "      <td>Mother's condition is going worse due to hepat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0  Lost my Job, Sick Mother and Paralysed Dad, In...   1042  g014wc   \n",
       "1  Why does the government come with a begging bo...    647  fxofyu   \n",
       "2  Mother's condition is going worse due to hepat...    756  g0zlly   \n",
       "\n",
       "                                                body             author  \\\n",
       "0  Hi....It's really tough time for everyone. I r...      sanand_satwik   \n",
       "1  We have floods, terrorist attacks, famines due...  TWO-WHEELER-MAFIA   \n",
       "2  Hi folks, I really appreciate the warm respons...      sanand_satwik   \n",
       "\n",
       "      flair                                                url  comms_num  \\\n",
       "0  AskIndia  https://www.reddit.com/r/india/comments/g014wc...        132   \n",
       "1  AskIndia  https://www.reddit.com/r/india/comments/fxofyu...        205   \n",
       "2  AskIndia  https://www.reddit.com/r/india/comments/g0zlly...         92   \n",
       "\n",
       "        created                                            comment  \\\n",
       "0  1.586742e+09   I'm a freelancer. Don't listen to the idiots ...   \n",
       "1  1.586448e+09   I don't understand why they don't use money f...   \n",
       "2  1.586871e+09   If anyone knows who is influential on Twitter...   \n",
       "\n",
       "                                             authors  \\\n",
       "0   hashedram diabapp xataari Aashayrao sarcrasti...   \n",
       "1   Kinky-Monk ak32009 fools_eye None DwncstSheep...   \n",
       "2   AlternativeDrop6 TheRobotsHaveCome lanky32 pl...   \n",
       "\n",
       "                                   combined_features  \n",
       "0  Lost my Job, Sick Mother and Paralysed Dad, In...  \n",
       "1  Why does the government come with a begging bo...  \n",
       "2  Mother's condition is going worse due to hepat...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(PATH/'data/data.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].astype(str)+df['body'].astype(str)+df['comment'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       AskIndia\n",
       "1       AskIndia\n",
       "2       AskIndia\n",
       "3       AskIndia\n",
       "4       AskIndia\n",
       "          ...   \n",
       "1795      Sports\n",
       "1796      Sports\n",
       "1797      Sports\n",
       "1798      Sports\n",
       "1799      Sports\n",
       "Name: flair, Length: 1800, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[['flair','title']]\n",
    "df[\"flair\"].apply(lambda x:str(x))\n",
    "# df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flairs=list(np.unique(df['flair']))\n",
    "df['flair']=df['flair'].apply(lambda x :all_flairs.index(x))\n",
    "all_flairs=list(np.unique(df['flair']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Lost my Job, Sick Mother and Paralysed Dad, In...\n",
       "1       Why does the government come with a begging bo...\n",
       "2       Mother's condition is going worse due to hepat...\n",
       "3       Men who are 30+ and have decided not to get ma...\n",
       "4       r/India: If money is no bar, would you prefer ...\n",
       "                              ...                        \n",
       "1795    Indian Women's and Men's Hockey Teams Seal Ber...\n",
       "1796    'It's humiliating for us': village disowns Dut...\n",
       "1797    Feroz Shah Kotla Renamed Arun Jaitley Stadium,...\n",
       "1798    Indian cricket fans are the most irritating ha...\n",
       "1799    Copy India's ambition to be the best: Ian Chap...\n",
       "Name: title, Length: 1800, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"title\"].apply(lambda x:str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df['title'],df['flair'],random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNhRJZ5gskWb"
   },
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U6q40Q4U6nY-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/ml/midas_internship/env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\n",
    "count_vect.fit(list(X_train) + list(X_test))\n",
    "X_train_count =  count_vect.transform(X_train) \n",
    "X_test_count = count_vect.transform(X_test)\n",
    "\n",
    "tfidf_vect.fit(list(X_train) + list(X_test))\n",
    "X_train_tfidf =  tfidf_vect.transform(X_train) \n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "\n",
    "tfidf_vect_ngram.fit(list(X_train) + list(X_test))\n",
    "X_train_tfidf_ngram =  tfidf_vect_ngram.transform(X_train) \n",
    "X_test_tfidf_ngram = tfidf_vect_ngram.transform(X_test)\n",
    "\n",
    "tfidf_vect_ngram_chars.fit(list(X_train) + list(X_test))\n",
    "X_train_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "X_test_tfidf_ngram_chars = tfidf_vect_ngram_chars.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1IxBQSfLnhU"
   },
   "outputs": [],
   "source": [
    "\n",
    "# convert to one_hot\n",
    "train_y_onehot = keras.utils.to_categorical(y_train, max(y_train)+1)\n",
    "valid_y_onehot = keras.utils.to_categorical(y_test, max(y_train)+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HHK658D3RvN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1350 samples, validate on 450 samples\n",
      "Epoch 1/3\n",
      "1350/1350 [==============================] - 1s 664us/step - loss: 2.0579 - accuracy: 0.2111 - val_loss: 1.8798 - val_accuracy: 0.3844\n",
      "Epoch 2/3\n",
      "1350/1350 [==============================] - 1s 435us/step - loss: 1.3630 - accuracy: 0.8222 - val_loss: 1.6557 - val_accuracy: 0.5422\n",
      "Epoch 3/3\n",
      "1350/1350 [==============================] - 1s 466us/step - loss: 0.9212 - accuracy: 0.9563 - val_loss: 1.5079 - val_accuracy: 0.6089\n",
      "NN, Count Vectors accuracy:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.27      0.35        44\n",
      "           1       0.43      0.61      0.50        44\n",
      "           2       0.90      0.71      0.79        51\n",
      "           3       0.37      0.67      0.48        52\n",
      "           4       0.85      0.80      0.83        51\n",
      "           5       0.50      0.55      0.52        49\n",
      "           6       0.64      0.65      0.65        55\n",
      "           7       0.71      0.43      0.54        51\n",
      "           8       0.95      0.72      0.82        53\n",
      "\n",
      "    accuracy                           0.61       450\n",
      "   macro avg       0.65      0.60      0.61       450\n",
      "weighted avg       0.66      0.61      0.62       450\n",
      "\n",
      "Train on 1350 samples, validate on 450 samples\n",
      "Epoch 1/3\n",
      "1350/1350 [==============================] - 0s 209us/step - loss: 2.1902 - accuracy: 0.1830 - val_loss: 2.1723 - val_accuracy: 0.2956\n",
      "Epoch 2/3\n",
      "1350/1350 [==============================] - 0s 108us/step - loss: 2.1351 - accuracy: 0.5467 - val_loss: 2.1324 - val_accuracy: 0.3844\n",
      "Epoch 3/3\n",
      "1350/1350 [==============================] - 0s 90us/step - loss: 2.0663 - accuracy: 0.6719 - val_loss: 2.0834 - val_accuracy: 0.4556\n",
      "NN, Word Level TF IDF Vectors accuracy:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.89      0.37        44\n",
      "           1       0.50      0.05      0.08        44\n",
      "           2       0.92      0.43      0.59        51\n",
      "           3       0.20      0.02      0.04        52\n",
      "           4       0.94      0.59      0.72        51\n",
      "           5       0.43      0.47      0.45        49\n",
      "           6       0.40      0.84      0.54        55\n",
      "           7       0.62      0.20      0.30        51\n",
      "           8       0.97      0.60      0.74        53\n",
      "\n",
      "    accuracy                           0.46       450\n",
      "   macro avg       0.58      0.45      0.43       450\n",
      "weighted avg       0.59      0.46      0.43       450\n",
      " \n",
      "Train on 1350 samples, validate on 450 samples\n",
      "Epoch 1/3\n",
      "1350/1350 [==============================] - 0s 138us/step - loss: 2.1913 - accuracy: 0.1519 - val_loss: 2.1815 - val_accuracy: 0.1578\n",
      "Epoch 2/3\n",
      "1350/1350 [==============================] - 0s 82us/step - loss: 2.1390 - accuracy: 0.4059 - val_loss: 2.1614 - val_accuracy: 0.1733\n",
      "Epoch 3/3\n",
      "1350/1350 [==============================] - 0s 92us/step - loss: 2.0821 - accuracy: 0.5148 - val_loss: 2.1367 - val_accuracy: 0.2356\n",
      "NN, Ngram Level TF IDF Vectors accuracy:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.89      0.29        44\n",
      "           1       0.50      0.05      0.08        44\n",
      "           2       0.20      0.14      0.16        51\n",
      "           3       0.14      0.06      0.08        52\n",
      "           4       0.80      0.08      0.14        51\n",
      "           5       0.37      0.41      0.39        49\n",
      "           6       0.27      0.49      0.35        55\n",
      "           7       0.00      0.00      0.00        51\n",
      "           8       1.00      0.08      0.14        53\n",
      "\n",
      "    accuracy                           0.24       450\n",
      "   macro avg       0.38      0.24      0.18       450\n",
      "weighted avg       0.39      0.24      0.18       450\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/ml/midas_internship/env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1350 samples, validate on 450 samples\n",
      "Epoch 1/3\n",
      "1350/1350 [==============================] - 0s 144us/step - loss: 2.1898 - accuracy: 0.1207 - val_loss: 2.1730 - val_accuracy: 0.1489\n",
      "Epoch 2/3\n",
      "1350/1350 [==============================] - 0s 109us/step - loss: 2.1379 - accuracy: 0.3096 - val_loss: 2.1325 - val_accuracy: 0.3133\n",
      "Epoch 3/3\n",
      "1350/1350 [==============================] - 0s 118us/step - loss: 2.0736 - accuracy: 0.4837 - val_loss: 2.0834 - val_accuracy: 0.3889\n",
      "NN, characters level tf-idf Vectors accuracy:              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.84      0.42        44\n",
      "           1       0.44      0.39      0.41        44\n",
      "           2       1.00      0.16      0.27        51\n",
      "           3       0.24      0.63      0.35        52\n",
      "           4       0.86      0.47      0.61        51\n",
      "           5       0.33      0.12      0.18        49\n",
      "           6       0.45      0.55      0.50        55\n",
      "           7       1.00      0.04      0.08        51\n",
      "           8       0.95      0.34      0.50        53\n",
      "\n",
      "    accuracy                           0.39       450\n",
      "   macro avg       0.62      0.39      0.37       450\n",
      "weighted avg       0.62      0.39      0.37       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def model_FF(X_train, ytrain, X_test, yvalid, hidden_size, epochs =3):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((X_train.shape[1], ), sparse=False)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(hidden_size, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(9, activation=\"softmax\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    classifier.fit(X_train, ytrain,\n",
    "                   validation_data=(X_test, yvalid),\n",
    "                  batch_size=256,\n",
    "                  epochs=epochs,\n",
    "                  shuffle = True)\n",
    "    # scores of the classifier\n",
    "    predictions = classifier.predict(X_test)\n",
    "    val_preds = predictions.argmax(axis=-1)\n",
    "#     accuracy = classifier.evaluate(X_test, yvalid, verbose=0)\n",
    "    auc = classification_report(y_test, val_preds)\n",
    "    return auc\n",
    "\n",
    "\n",
    "# NN Classifier on Count Vectors\n",
    "accuracy = model_FF(X_train_count, train_y_onehot, X_test_count, valid_y_onehot, hidden_size=100)\n",
    "print(\"NN, Count Vectors accuracy:%s\"% (accuracy))\n",
    "\n",
    "# NN Classifier on Word Level TF IDF Vectors\n",
    "accuracy = model_FF(X_train_tfidf, train_y_onehot, X_test_tfidf, valid_y_onehot, 100)\n",
    "print(\"NN, Word Level TF IDF Vectors accuracy:%s \"% (accuracy))\n",
    "\n",
    "# NN Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = model_FF(X_train_tfidf_ngram, train_y_onehot, X_test_tfidf_ngram, valid_y_onehot, 100)\n",
    "print(\"NN, Ngram Level TF IDF Vectors accuracy:%s\"% (accuracy))\n",
    "\n",
    "# NN Classifier on characters level tf-idf\n",
    "accuracy = model_FF(X_train_tfidf_ngram_chars, train_y_onehot, X_test_tfidf_ngram_chars, valid_y_onehot, 100)\n",
    "print(\"NN, characters level tf-idf Vectors accuracy:%s\"% (accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3587vzP3_n-M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib\r\n",
      "  Referenced from: /usr/local/bin/wget\r\n",
      "  Reason: image not found\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip -P \"../weight_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dqizwGXFAzSo",
    "outputId": "e1766915-37a7-4d37-8688-4fe04b57553f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/My Drive/models/wiki-news-300d-1M.vec.zip\n",
      "  inflating: /content/drive/My Drive/models/wiki-news-300d-1M.vec  \n"
     ]
    }
   ],
   "source": [
    "!unzip '/content/drive/My Drive/models/wiki-news-300d-1M.vec.zip' -d '/content/drive/My Drive/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuJ3PjVtBD70"
   },
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('/content/drive/My Drive/models/wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(xtrain)\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(xtrain), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(xvalid), maxlen=70)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQtkdWPE38Lm"
   },
   "outputs": [],
   "source": [
    "def cnn(xtrain, ytrain, xvalid, yvalid, eps = 3):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 4, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(2, activation=\"softmax\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=eps)\n",
    "    predictions = model.predict(xvalid)\n",
    "    val_preds = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score=final_score(getf1_score(np.argmax(yvalid,axis=1),val_preds),f1_weight(yvalid,val_preds))\n",
    "    return accuracy, f1score\n",
    "  \n",
    "accuracy, f1score = cnn(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"CNN, Word Embeddings acuuracy accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "oOqY5Eb_BZHw",
    "outputId": "e3cdbc29-dcff-4a25-ec86-eef339d3ae7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "8500/8500 [==============================] - 6s 701us/step - loss: 0.5737 - acc: 0.7453\n",
      "Epoch 2/4\n",
      "8500/8500 [==============================] - 5s 570us/step - loss: 0.4996 - acc: 0.7639\n",
      "Epoch 3/4\n",
      "8500/8500 [==============================] - 5s 566us/step - loss: 0.4179 - acc: 0.8114\n",
      "Epoch 4/4\n",
      "8500/8500 [==============================] - 5s 568us/step - loss: 0.3885 - acc: 0.8308\n",
      "LSTM, Word Embeddings accuracy:0.7297297297297297     f1 score: 0.6862745098039216\n"
     ]
    }
   ],
   "source": [
    "#lstm unidirectional 128\n",
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer1 = layers.LSTM(128)(embedding_layer)\n",
    "    dropout1 = layers.Dropout(0.5)(lstm_layer1)\n",
    "    #lstm_layer2 = layers.LSTM(128)(dropout1)\n",
    "    #dropout2 = layers.Dropout(0.5)(lstm_layer2)\n",
    "    # Add the output Layers\n",
    "    output_layer = layers.Dense(2, activation=\"softmax\")(dropout1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=4)\n",
    "    \n",
    "    predictions = model.predict(xvalid)\n",
    "    val_preds = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score=final_score(getf1_score(np.argmax(yvalid,axis=1),val_preds),f1_weight(yvalid,val_preds))\n",
    "    return accuracy, f1score\n",
    "    \n",
    "accuracy, f1score = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "i_VLSpknMkdK",
    "outputId": "b0d5f093-968f-4e54-c282-8d7ec3d3651b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "8500/8500 [==============================] - 13s 2ms/step - loss: 0.5733 - acc: 0.7446\n",
      "Epoch 2/4\n",
      "8500/8500 [==============================] - 10s 1ms/step - loss: 0.4805 - acc: 0.7771\n",
      "Epoch 3/4\n",
      "8500/8500 [==============================] - 10s 1ms/step - loss: 0.4096 - acc: 0.8206\n",
      "Epoch 4/4\n",
      "8500/8500 [==============================] - 10s 1ms/step - loss: 0.3824 - acc: 0.8355\n",
      "LSTM, Word Embeddings accuracy:0.7668918918918919     f1 score: 0.7676767676767677\n"
     ]
    }
   ],
   "source": [
    "#lstm bidirectional 128\n",
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer1 =  layers.Bidirectional(layers.LSTM(128))(embedding_layer)\n",
    "    dropout1 = layers.Dropout(0.5)(lstm_layer1)\n",
    "    #lstm_layer2 = layers.LSTM(128)(dropout1)\n",
    "    #dropout2 = layers.Dropout(0.5)(lstm_layer2)\n",
    "    # Add the output Layers\n",
    "    output_layer = layers.Dense(2, activation=\"softmax\")(dropout1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=4)\n",
    "    \n",
    "    predictions = model.predict(xvalid)\n",
    "    val_preds = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score=final_score(getf1_score(np.argmax(yvalid,axis=1),val_preds),f1_weight(yvalid,val_preds))\n",
    "    return accuracy, f1score\n",
    "    \n",
    "accuracy, f1score = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zq_aJ_Aus3-S"
   },
   "outputs": [],
   "source": [
    "# wiki bidir lstm 256\n",
    "\n",
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer1 =  layers.Bidirectional(layers.LSTM(256))(embedding_layer)\n",
    "    dropout1 = layers.Dropout(0.5)(lstm_layer1)\n",
    "    #lstm_layer2 = layers.LSTM(128)(dropout1)\n",
    "    #dropout2 = layers.Dropout(0.5)(lstm_layer2)\n",
    "    # Add the output Layers\n",
    "    output_layer = layers.Dense(2, activation=\"softmax\")(dropout1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=4)\n",
    "    \n",
    "    predictions = model.predict(xvalid)\n",
    "    val_preds = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "#     metrcs=sklearn.metrics.precision_recall_fscore_support(np.argmax(yvalid,axis=1),val_preds)\n",
    "    f1score=final_score(getf1_score(np.argmax(yvalid,axis=1),val_preds),f1_weight(yvalid,val_preds))\n",
    "    return accuracy, f1score\n",
    "    \n",
    "accuracy, f1score = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RiMmDTHx0y8G",
    "outputId": "b660ff34-ece9-4e89-c236-d45940d5a459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM, Word Embeddings accuracy:0.7432432432432432     f1 score: 0.7110266159695818\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Gfvilfq1YLg"
   },
   "outputs": [],
   "source": [
    "# wiki bidir lstm 1024\n",
    "\n",
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer1 =  layers.Bidirectional(layers.LSTM(1024))(embedding_layer)\n",
    "    dropout1 = layers.Dropout(0.5)(lstm_layer1)\n",
    "#     lstm_layer2 = layers.Bidirectional(layers.LSTM(256))(dropout1)\n",
    "#     dropout2 = layers.Dropout(0.5)(lstm_layer2)\n",
    "    # Add the output Layers\n",
    "    output_layer = layers.Dense(2, activation=\"softmax\")(dropout1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=3)\n",
    "    \n",
    "    predictions = model.predict(xvalid)\n",
    "    val_preds = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "#     metrcs=sklearn.metrics.precision_recall_fscore_support(np.argmax(yvalid,axis=1),val_preds)\n",
    "    f1score=final_score(getf1_score(np.argmax(yvalid,axis=1),val_preds),f1_weight(yvalid,val_preds))\n",
    "    return accuracy, f1score\n",
    "    \n",
    "accuracy, f1score = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2dv5PbEszj4"
   },
   "source": [
    "**Glove Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-XQNKO5YOcb"
   },
   "outputs": [],
   "source": [
    "!unzip '/content/drive/My Drive/glove.6B.zip' -d '/content/drive/My Drive/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OtIGuNCzaDur"
   },
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/content/drive/My Drive/models/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y964IA6rtTM6"
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 20000\n",
    "tokenizer = text.Tokenizer(num_words= vocabulary_size)\n",
    "\n",
    "tokenizer.fit_on_texts(xtrain)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(xtrain), maxlen=50)\n",
    "valid_seq_x = sequence.pad_sequences(tokenizer.texts_to_sequences(xvalid), maxlen=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "roz7jIyRsv_k"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhawSXdut_cT"
   },
   "outputs": [],
   "source": [
    "# convert to one_hot\n",
    "train_y_onehot = keras.utils.to_categorical(ytrain, 2)\n",
    "valid_y_onehot = keras.utils.to_categorical(yvalid, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwqkmjSEtAu5"
   },
   "outputs": [],
   "source": [
    "def lstm(xtrain, ytrain, xvalid, yvalid, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((50, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(vocabulary_size, 100, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer1 = layers.LSTM(128)(embedding_layer)\n",
    "    dropout1 = layers.Dropout(0.5)(lstm_layer1)\n",
    "    #lstm_layer2 = layers.LSTM(128)(dropout1)\n",
    "    #dropout2 = layers.Dropout(0.5)(lstm_layer2)\n",
    "    # Add the output Layers\n",
    "    output_layer = layers.Dense(2, activation=\"softmax\")(dropout1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xtrain, ytrain,\n",
    "              batch_size=256,\n",
    "              epochs=2)\n",
    "    \n",
    "    predictions = model.predict(xvalid)\n",
    "    val_preds = predictions.argmax(axis=-1)\n",
    "    accuracy = model.evaluate(xvalid, yvalid, verbose=0)\n",
    "    f1score=final_score(getf1_score(np.argmax(yvalid,axis=1),val_preds),f1_weight(yvalid,val_preds))\n",
    "    return accuracy, f1score\n",
    "    \n",
    "accuracy, f1score = lstm(train_seq_x, train_y_onehot, valid_seq_x, valid_y_onehot)\n",
    "print(\"LSTM, Word Embeddings accuracy:%s     f1 score: %s\"% (accuracy[1], f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jze-dR6PuBT6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "baselines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
